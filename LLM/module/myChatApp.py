
import streamlit as st
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.memory import ConversationBufferMemory
from langchain.chains import LLMChain
from openai import OpenAI

#
chat_model = ChatOpenAI(model_name='gpt-3.5-turbo',
                        api_key = st.secrets["OPENAI_API_KEY"],
                        temperature = 0.5
)

prompt = PromptTemplate(input_variables = ["chat_history", "question"],
                        template = """You are a AI assistant.
                                      You are currently haing a conversation with a human.
                                      Answer the Question.
                                      chat_history : {chat_history},
                                      Human : {question},
                                      AI assistant : 
                                      """
                       )

# ì¼ë°˜ì ì¸ ì½”ë“œì—ì„œëŠ” memoryê°ì²´ë¥¼ ìƒì„±í•˜ë©´ ëŒ€í™” ë‚´ìš©ë“¤ì„ ìë™ìœ¼ë¡œ ê¸°ì–µí•˜ì§€ë§Œ 
# streamlitì—ì„œëŠ” ì›¹ì—ì„œ ìš”ì²­, ë‹µì„ ìˆ˜í–‰í•˜ê¸° ë•Œë¬¸ì— ë”°ë¡œ ì§€ì •í•´ë‘ì§€ ì•Šìœ¼ë©´ ë‹¤ ì´ˆê¸°í™” ë¨

# session_state : ë”•ì…”ë„ˆë¦¬ì™€ ìœ ì‚¬í•œ êµ¬ì¡°ë¡œ ëª¨ë“  ìë£Œí˜• ë° ê°ì²´ë¥¼ ì§€ì •í•  ìˆ˜ ìˆê³  []ì™¸ì— .ìœ¼ë¡œë„ keyì— ì ‘ê·¼ ê°€ëŠ¥
#                  ì•±ì„ ì¬ì‹¤í–‰í•´ë„ ì‚¬ìš©ì ì…ë ¥ì´ë‚˜ ê³„ì‚°ê²°ê³¼ ë“± ë³€ìˆ˜ì˜ ìƒíƒœë¥¼ ì§€ì†ì ìœ¼ë¡œ ê´€ë¦¬í•´ì•¼ í•  ë•Œ ì‚¬ìš©í•¨


# ì²˜ìŒë¶€í„° ì§€ì •í•˜ì§€ ì•Šê³  not inì„ ì“°ëŠ” ì´ìœ ëŠ” ì½”ë“œ ì¬ì‹¤í–‰ ì‹œ ì§€ì •ëœ ê°’ì´ ì´ˆê¸°í™” ë˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•¨
if "pre_memory" not in st.session_state:
    st.session_state.pre_memory = ConversationBufferMemory(memory_key="chat_history",
                                                           return_messages=True
                                                          )

llm_chain = LLMChain(llm=chat_model,
                     prompt=prompt,
                     memory=st.session_state.pre_memory # CBMemoryê°ì²´ê°€ ì…ë ¥ ë¨
                    )

# LLMì— ìš”ì²­í•˜ê³  ì‘ë‹µë°›ì„ ìˆ˜ ìˆëŠ” chatí˜•íƒœë¡œ messagesë¥¼ ê´€ë¦¬
if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role" : "assistant", "content" : "ì•ˆë…•í•˜ì„¸ìš” ì €ëŠ” AI Assistantì…ë‹ˆë‹¤."}
    ]

# ------------------------ ì›¹í‘œì‹œ ë¶€ë¶„ -----------------------------------
st.title("ë‚˜ì˜ ì‘ê³  ì†Œì¤‘í•œ GPT ì±—ë´‡ğŸ˜Š")

# ë°˜ë³µë¬¸ìœ¼ë¡œ session_state.messagesì— ìˆëŠ” ëª¨ë“  ëŒ€í™” ê¸°ë¡ì— ì ‘ê·¼
for message in st.session_state.messages:
    # chat_message : ë©”ì„¸ì§€ì˜ ë°œì‹ ì role(assistant, user)ì— ë”°ë¼ UIë¥¼ êµ¬ë¶„í•˜ì—¬ ì¶œë ¥
    #                assistant, userëŠ” ë””í´íŠ¸ë¡œ ì„¤ì •ë˜ì–´ ìˆìœ¼ë©° ë‹¤ë¥¸ roleì„ ì¶”ê°€í•˜ë ¤ë©´ html ì½”ë“œ ì‘ì„±ì´ í•„ìš”
    with st.chat_message(message["role"]):
        st.write(message["content"])

# chat_input : ì±„íŒ…ë©”ì„¸ì§€ ì…ë ¥ UIê°€ ìƒì„±ë˜ë©° ì‚¬ìš©ìê°€ ì…ë ¥í•œ í…ìŠ¤íŠ¸ë¥¼ ë°›ì•„ì˜¤ëŠ” í•¨ìˆ˜
user_prompt = st.chat_input()

# ì‚¬ìš©ìê°€ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•œë‹¤ë©´!
if user_prompt is not None:
    st.session_state.messages.append({"role" : "user", "content" : user_prompt})
    with st.chat_message("user"):  # userí˜•íƒœë¡œ ë©”ì„¸ì§€ ì°½ UI ì¶œë ¥
        st.write(user_prompt)      # ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì§ˆë¬¸ ì¶œë ¥

# ë§ˆì§€ë§‰ ë©”ì„¸ì§€ì˜ roleì´ assistantê°€ ì•„ë‹ˆë¼ë©´ ìƒˆë¡œìš´ ë‹µë³€ì„ ìƒì„±í•˜ê³  session_stateì— ì¶”ê°€
# (assistantê°€ ë³¸ì¸ì˜ ì‘ë‹µì—ë„ ê³„ì† ëŒ€ë‹µí•˜ëŠ” ìë¬¸ìë‹µì„ ë°©ì§€í•˜ê¸° ìœ„í•¨)
if st.session_state.messages[-1]["role"] != "assistant" :
    # assistant ë©”ì„¸ì§€ UI ìƒì„±
    with st.chat_message("assistant") :
        # 
        with st.spinner("Loading..") :
            try :
                ai_response = llm_chain.predict(question=user_prompt)
                st.write(ai_response)
                # ëª¨ë¸ì˜ ì‘ë‹µë„ ì„¸ì…˜ì˜ messagesì— ì¶”ê°€í•˜ì—¬ ê´€ë¦¬
                st.session_state.messages.append({"role" : "assistant", "content" : ai_response})
            except Exception as e:
                st.error(f"LLM ì—ëŸ¬ ë°œìƒ : {e}")

                 
#------------------------------------------------------------------------
