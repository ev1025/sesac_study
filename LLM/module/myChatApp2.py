
import streamlit as st
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.memory import ConversationBufferMemory
from langchain.chains import LLMChain
from openai import OpenAI

# -------------------------- ì‚¬ì „ ì„¤ì • ------------------------------
# ì§ˆì˜ê°€ ì ì €í•œ ì½˜í…ì¸ ì¸ì§€ í™•ì¸í•˜ëŠ” ì‚¬ìš©ì ì •ì˜ í•¨ìˆ˜(moderation ì ìš© ëª¨ë¸)
def moderate_message(message):
    client = OpenAI(api_key=st.secrets["OPENAI_API_KEY"])
    response = client.moderations.create(model="text-moderation-latest",
                                        input=message
                                        )
    moderation_result = response.results[0]

    # ë©”ì„¸ì§€ê°€ ìœ í•´í•˜ë‹¤ê³  íŒë‹¨ë˜ëŠ” ê²½ìš°(flaggedê°€ Trueì¸ ê²½ìš°!)
    if moderation_result.flagged :
        category_type = dict(moderation_result.categories)            # ë”•ì…”ë„ˆë¦¬ ë³€í™˜
        categories = [i for i,j in category_type.items() if j==True]  # ë”•ì…”ë„ˆë¦¬ì—ì„œ Trueê°’ ì¶”ì¶œ
        return False, categories
        
    return True, None
    # Noneì€ ì—†ì–´ë„ ë˜ì§€ë§Œ returnì´ ì—¬ëŸ¬ê°œì¸ ê²½ìš° ë°˜í™˜ ê°’ì˜ í¬ê¸°ë¥¼ ë§ì¶°ì£¼ëŠ” ê²ƒ


# LLMChain í´ë˜ìŠ¤ë¥¼ ìƒì†ë°›ì€ ì‚¬ìš©ì ì •ì˜ í´ë˜ìŠ¤ì¸ ModeratedLLMChain ì„ ì–¸
# LLMChain ë‚´ë¶€ì—ì„œ Moderationì„ ì§€ì›í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ìƒì† ë°›ì•„ì„œ ì¶”ê°€ì½”ë“œ ì‘ì„±)
class ModeratedLLMChain(LLMChain) :
    # ì „ë‹¬ ë©”ì„¸ì§€ê°€ ì ì ˆí•œì§€ í™•ì¸í•˜ê¸° ìœ„í•´ moderate_messageí•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ê³  ì‘ë‹µ ìƒì„± ë°
    # ìƒí™©ì— ë”°ë¥¸ ì¶œë ¥ ë¬¸êµ¬ ì‘ì„±
    def moderate_and_generate(self, user_input):
        # ì‚¬ìš©ìì˜ ì •ì˜ì— ëŒ€í•œ Moderation
        is_safe, categories = moderate_message(user_input)

        # moderate_messageê²°ê³¼ë¡œ ìœ í•´í•˜ë‹¤ê³  íŒë‹¨ë˜ë©´ is_safeì— Falseê°’ ë°˜í™˜
        # ì¦‰, ìœ í•´í•˜ë‹¤ê³  íŒë‹¨ì´ ëœë‹¤ë©´ ì•„ë˜ ì¡°ê±´ë¬¸ ì‹¤í–‰
        if not is_safe:
            st.write(f"ì‚¬ìš©ìì˜ ë©”ì„¸ì§€ê°€ ë¶€ì ì ˆí•˜ì—¬ ì°¨ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.ğŸ˜’ : {categories}")
            return "" # returnê°’ì´ ì—†ìœ¼ë©´ Noneì´ í™”ë©´ì— ì¶œë ¥ë˜ê¸° ë•Œë¬¸ì— ì´ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•œ ì½”ë“œ

        # ìƒì†ë°›ì€ LLMChainí´ë˜ìŠ¤ì˜ predict ë©”ì†Œë“œë¡œ ì‘ë‹µ ì‹¤í–‰
        response = self.predict(question=user_input)

        # ëª¨ë¸ ì‘ë‹µì— ëŒ€í•œ Moderation
        is_safe_ai, categories_ai = moderate_message(response)
        if not is_safe_ai:
            st.write(f"AIì˜ ì‘ë‹µì´ ë¶€ì ì ˆí•˜ì—¬ ì°¨ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.ğŸ˜’ : {categories}")
            return " "
            
        return response

chat_model = ChatOpenAI(model_name='gpt-3.5-turbo',
                        api_key = st.secrets["OPENAI_API_KEY"],
                        temperature = 0.5
                       )

prompt = PromptTemplate(input_variables = ["chat_history", "question"],
                        template = """You are a AI assistant.
                                      You are currently haing a conversation with a human.
                                      Answer the Question.
                                      chat_history : {chat_history},
                                      Human : {question},
                                      AI assistant : 
                                      """
                       )

if "pre_memory" not in st.session_state:
    st.session_state.pre_memory = ConversationBufferMemory(memory_key="chat_history",
                                                           return_messages=True
                                                          )

# ìœ„ì—ì„œ ì •ì˜í•œ í´ë˜ìŠ¤
moderated_chain = ModeratedLLMChain(llm=chat_model,
                                    prompt=prompt,
                                    memory=st.session_state.pre_memory # CBMemoryê°ì²´ê°€ ì…ë ¥ ë¨
                                   )

if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role" : "assistant", "content" : "ì•ˆë…•í•˜ì„¸ìš” ì €ëŠ” ì¢€ ë” í–¥ìƒëœ AI Assistantì…ë‹ˆë‹¤."}
    ]

# ------------------------ ì›¹í‘œì‹œ ë¶€ë¶„ -----------------------------------
st.title("ë‚˜ì˜ ì‘ê³  ì†Œì¤‘í•œ GPT ì±—ë´‡ğŸ˜Š")

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])

user_prompt = st.chat_input()

if user_prompt is not None:
    st.session_state.messages.append({"role" : "user", "content" : user_prompt})
    with st.chat_message("user"):  # userí˜•íƒœë¡œ ë©”ì„¸ì§€ ì°½ UI ì¶œë ¥
        st.write(user_prompt)      # ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì§ˆë¬¸ ì¶œë ¥


if st.session_state.messages[-1]["role"] != "assistant" :
    # assistant ë©”ì„¸ì§€ UI ìƒì„±
    with st.chat_message("assistant") :
        # 
        with st.spinner("Loading..") :
            try :
                ai_response = moderated_chain.moderate_and_generate(user_prompt) # í´ë˜ìŠ¤ì—ì„œ ê°€ì ¸ì˜¤ê¸°
                st.write(ai_response)
                st.session_state.messages.append({"role" : "assistant", "content" : ai_response})
            except Exception as e:
                st.error(f"LLM ì—ëŸ¬ ë°œìƒ : {e}")

#------------------------------------------------------------------------
