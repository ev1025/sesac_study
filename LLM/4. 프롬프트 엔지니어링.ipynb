{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c44790a3-96f0-4edd-af61-d0d40392db32",
   "metadata": {},
   "source": [
    "## 기본적인 Prompt 구조 이해\n",
    "prompt에는 3가지 종류의 역할(role)이 존재\n",
    "\n",
    "1. sysytem prompt : 사용자 prompt를 입력받기 이전에 정의되는 전제 및규칙\n",
    "2. user prompt : 사용자가 LLM에게 실제로 요청하는 prompt\n",
    "3. assistant prompt : LLM이 응답하는 prompt\n",
    "\n",
    "#### system prompt란?\n",
    "user prompt를 LLM에게 전달하기 전, 관련된 맥락이나 응답지침 등을 설정하는 prompt\n",
    "- 출력 형태 지정(ex. JSON 딕셔너리 등)\n",
    "- 페르소나(투자 전문가, 예술가 등) 및 어조 설정(공손한, 전문적인 등)\n",
    "- 모델이 지켜야할 규칙들 설정\n",
    "- 기타 base가 되는 외부 정보 및 지식 주입\n",
    "\n",
    "\n",
    "#### ChatGPT를 포함한 웬만하면 LLM모델 서비스들은 prompt 입력시 기본적으로 지정된 system prompt가 내부에서 동작하고 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba4c1137-31bc-45db-ac53-e9bd9b946708",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from getpass import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e34505f-5759-4330-a839-c3a84ddadd0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input your key ········\n"
     ]
    }
   ],
   "source": [
    "MY_API_KEY = getpass.getpass(\"input your key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a1674f7-d061-4ba3-9df8-1e4b0d9abb32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "하늘은 왜 하늘색인지 궁금하시군요! 하늘은 우리가 보는 것과 같이 파란색으로 보이는데요, 이것은 태양빛이 하늘과 대기에 닿았을 때 일어나는 현상 때문이에요.\n",
      "\n",
      "태양빛은 여러 색으로 이루어져 있는데, 그 중에서 파란색 빛은 다른 색보다 더 짧은 파장을 가지고 있어요. 그래서 하늘과 대기 속의 먼지와 수증기 등이 파란색 빛을 더 많이 흡수하고 흩뿌리게 되는데요. 이런 현상 때문에 우리가 보는 하늘은 파란색으로 보이게 되는거죠!\n",
      "\n",
      "그래서 하늘은 하늘색인 것이에요. 파란색 빛이 더 많이 흩뿌려져서 우리 눈에 파란색으로 보이는 거죠. 재미있는 현상이죠!\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI(api_key = MY_API_KEY)\n",
    "\n",
    "completion  = client.chat.completions.create(model = 'gpt-3.5-turbo',\n",
    "                                             messages = [{'role' : 'system',\n",
    "                                                          'content' : '당신은 물리학 선생님입니다. 초등학생에게 설명하듯 아주 쉽고 친근하게 설명해야 합니다.'},\n",
    "                                                         {'role' : 'user',\n",
    "                                                          'content': '왜 하늘은 하늘색인가요?'}],\n",
    "                                             temperature = 0\n",
    "                                            )\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c56cde-ba6e-4048-ae85-4b18c5add2f4",
   "metadata": {},
   "source": [
    "- 텍스트가 짧다면 user prompt에 system prompt 내용을 같이 작성해도 무방함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ff896b-9921-4163-827b-544bb86568c1",
   "metadata": {},
   "source": [
    "### Stream 객체 살펴보기\n",
    "- stream = True로 지정시 GPT가 문장을 모두 완성하기 전에 각 토큰별로 완성되는대로 바로바로 보여줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "edbffd04-8e8c-4d99-9e3e-da0feb77805b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "하늘은 하늘색인 이유는 대기 중의 분자들이 푸른 빛을 흡수하고 다른 색상의 빛을 반사하기 때문입니다. 태양 빛은 다양한 색상의 빛으로 구성되어 있지만, 대기 중의 분자들이 푸른 빛을 흡수하고 다른 색상의 빛을 반사하게 되어 우리 눈에는 하늘이 푸르게 보이는 것입니다. 이러한 현상을 산란이라고 하며, 이로 인해 하늘은 하늘색으로 보이게 됩니다."
     ]
    }
   ],
   "source": [
    "# stream=True로 설정하게 되면 응답 객체가 ChatCompletion이 아니라 ChatCompletionChunk로 변환됨\n",
    " # Chunk는 토큰들의 의미있는 집합 \n",
    "completion_stream  = client.chat.completions.create(model = 'gpt-3.5-turbo',\n",
    "                                                    messages = [{'role' : 'user',\n",
    "                                                                 'content': '왜 하늘은 하늘색인가요?'}],\n",
    "                                                    temperature = 0,\n",
    "                                                    stream=True\n",
    "                                                   )\n",
    "\n",
    "for i in completion_stream:\n",
    "    # print(i)\n",
    "    content = i.choices[0].delta.content\n",
    "\n",
    "    # content가 비어있지 않은 경우 \n",
    "    if content is not None:\n",
    "        print(content, end = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd73fcc-8c7b-4a95-a377-4eb22d053c9e",
   "metadata": {},
   "source": [
    "### LLM의 task에 따른 평가지표들을 알아보고 대화 및 Q&A task에 실제로 적용해보자\n",
    "#### 전통적인 Language Model 평가 지표\n",
    "1. MMLU(Massive Multitask Language Understanding)\n",
    " - 다양한 분야에 대한 질문 후 정답을 찾아내게 하는 객관식 시험\n",
    "2. HellaSwag\n",
    " - 문장들을 주고 이어지는 마지막 문장으로 가장 적합한 문장들 4개중 하나를 고르는 시험\n",
    "\n",
    "#### 위 평가지표들은 범용 Language Model에 대한 평가 방법론들이라 현재 우리가 진행할 대화 및 Q&A에는 적합하지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f33c219-e0af-43ef-9490-d1a821e91eb7",
   "metadata": {},
   "source": [
    "### 대화 및 Q&A task에 적합한 평가 방식\n",
    "1. **Human Based Evaluation** - 사람이 직접 평가하는 방법\n",
    "2. **Model Based Evaluation** - LLM이 평가하는 방법\n",
    "3. **Code Based Evaluation** - 코드로 평가하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e0c59d-f2ed-48e6-9b9e-4c8971f0d1d4",
   "metadata": {},
   "source": [
    "### 1) Human Based Evaluation \n",
    "- 전문가 블라인드 A/B테스트(각기 다른 LLM의 2가지 답변 중 더 좋은 답변을 사람이 선택)\n",
    "- 명확한 결과로 성능을 판단하기 쉬움\n",
    "- 많은 인력에 따른 비용과 시간이 필요함\n",
    "\n",
    "#### LMSys사 Chatbot Arena 평가방법\n",
    "- 대표적인 Human Based 방식으로 동일한 질문에 대해 2개의 모델의 답변을 보고 승/패/무 투표 이후 모델명을 공개하는 방식\n",
    "- 사이트 : https://chat.lmsys.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfcee3c-300e-4e1b-9b23-3e28ac926248",
   "metadata": {},
   "source": [
    "### 2) Model Based Evaluation\n",
    "고성능 LLM(GPT-4o이상)을 통해 평가하는 방법\n",
    "- 실제로 사람이 평가하는 것과 굉장히 유사하다는 논물 결과들이 나오고 있음\n",
    "\n",
    "평가하는 방식에는 3가지가 존재\n",
    "1. **Pairwise Comparison**\n",
    "-  2개의 평가받을 모델에 같은 질문을 하고, 고성능 모델이 2개의 답변을 받아 둘 중 어떤 답변이 더 좋은지 또는 무승부인지를 출력\n",
    "2. **Single Answer Grading**\n",
    "- 질문과 답변이 있을 때 모델이 답변에 점수를 매기는 것\n",
    "3. **Reference-Guided Grading**\n",
    "- 예시 답변을 주고 이와 비교하여 +,-로 상대적인 점수를 매기는 방식"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ec02ba-ef28-487b-8eba-073fbf22adca",
   "metadata": {},
   "source": [
    "### 3) Code Based Evaluation\n",
    "우리에게 익숙한 코드/로직을 통한 평가 방법\n",
    "- Accuray, Precision, Recall, F1-Score\n",
    "- ROUGE(Recall-Oriented Understudy for Gisting Evaluation) : 자연어 생성 및 요약을 평가\n",
    "- BLEU(Bilingual Evaluation Understudy) : 번역, 요약, 자연어 생성 등을 평가\n",
    "\n",
    "단, Human based 및 Model based에 비해 실제 사용자들의 만족과는 다소 거리가 있을 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d4dd67-942d-42cb-a050-747a96c6f190",
   "metadata": {},
   "source": [
    "### 최신 논문을 참조하여 대화 및 Q&A task에 적합한 평가 방법에 대한 힌트를 얻어보자\n",
    "- 논문명 : Judging LLM-as-a-Judge with MT-Bench and ChatbotArena\n",
    "- GPT-4로 평가 진행(그때 당시 가장 성능이 좋은 LLM)\n",
    "- 해당 논문의 핵심은 LLM의 평가 결과가 사람의 평가 결과와 일치율이 점점 높아지고 있다는 것\n",
    "- 이는 기계의 판단이 인간의 판단에 도움이 되며, 기계를 통한 자동화된 평가 프로세스 개발이 가능하다는 것을 시사함\n",
    "- 실제로 해당 논문에서 인간의 75%가 기계의 판단이 합리적이라 생각했고 34%가 자신의 선택을 바꿀 의향이 있다고 응답함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d968ab4d-97ec-4d10-8c45-18c84daf8d83",
   "metadata": {},
   "source": [
    "### benchmark : 다양한 시스템, 모델, 소프트웨어 등을 평가하고 비교하기 위해 사용되는 표준 테스트 세트 또는 기준\n",
    "\n",
    "### MT-bench(Multi-turn benh)\n",
    "- multi-turn(이어지는 대화) 능력을 평가하는 벤치마크\n",
    "- 먼저 58명의 전문가 모델의 응답을 평가하고 LLM을 심판으로 사용하여 사람의 평가와 일치하는지 검증하는 방식\n",
    "- ※ multi-turn 대화 : MT-bench에는 8개 카테고리(작문, 역할놀이, 추출, 추론, 수학, 코딩, 지식1(물리/수학), 지식2(인문/사회))의 80개의 고품질 질문으로 구성되어 있고, 각 질문은 여러차례의 응답을 요구하여 모델의 대화 지속능력을 평가함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "164e9d9b-fa8f-485f-b506-594199ec6e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = '하늘은 왜 하늘색인가요?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "caaf7c50-9004-4b5f-acb8-a7bb8fc26b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "하늘은 파란색으로 보이는데, 이는 태양 빛이 대기 중의 공기 분자들과 상호 작용하여 발생하는 현상입니다. 태양 빛은 다양한 색상의 빛으로 구성되어 있지만, 대기 중의 공기 분자들이 파란색 빛을 더 많이 흡수하고 다른 색상의 빛을 덜 흡수하기 때문에 하늘이 파란색으로 보이는 것입니다. 이러한 현상을 산란이라고 하며, 이로 인해 하늘은 파란색으로 보이게 됩니다.\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(model = 'gpt-3.5-turbo',\n",
    "                                            messages = [{'role' : 'user',\n",
    "                                                         'content' : question}],\n",
    "                                            temperature = 0\n",
    "                                           )\n",
    "answer_a = completion.choices[0].message.content\n",
    "print(answer_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4b01b66b-6e42-454a-bd9b-fd295984484d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "하늘의 색깔이 하늘색, 즉 파란색으로 보이는 이유는 대기 중의 분자들과 작은 입자들이 태양 빛을 산란시키기 때문입니다. 이 현상을 '레일리 산란'이라고 합니다.\n",
      "\n",
      "태양에서 오는 빛은 여러 가지 색의 빛으로 구성되어 있으며, 이 빛들은 파장의 길이에 따라 다르게 산란됩니다. 레일리 산란은 파장이 짧은 빛 (예: 파란색, 보라색)이 파장이 긴 빛 (예: 빨간색, 주황색)보다 더 많이 산란되는 현상입니다. 파란색 빛의 파장은 매우 짧기 때문에 대기 중을 통과할 때 다른 색의 빛보다 훨씬 더 많이 산란되어, 우리 눈에는 하늘이 파란색으로 보이게 됩니다.\n",
      "\n",
      "그러나 해가 지평선에 가까울 때, 즉 일출이나 일몰 때에는 하늘이 빨강, 주황, 노랑과 같은 따뜻한 색으로 보이는데, 이는 태양 빛이 대기를 통과하는 거리가 훨씬 길어져서 파란색과 보라색 빛이 대기에 의해 더 많이 산란되고, 빨간색과 주황색 빛이 상대적으로 덜 산란되어 우리 눈에 도달하기 때문입니다.\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(model = 'gpt-4-turbo',\n",
    "                                            messages = [{'role' : 'user',\n",
    "                                                         'content' : question}],\n",
    "                                            temperature = 0\n",
    "                                           )\n",
    "answer_b = completion.choices[0].message.content\n",
    "print(answer_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3a251654-fdd3-4d32-84e8-a6cfa4295bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[System]\n",
      "Please act as an impartial judge and evaluate the quality of the responses provided by two\n",
      "AI assistants to the user question displayed below. You should choose the assistant that\n",
      "follows the user’s instructions and answers the user’s question better. Your evaluation\n",
      "should consider factors such as the helpfulness, relevance, accuracy, depth, creativity,\n",
      "and level of detail of their responses. Begin your evaluation by comparing the two\n",
      "responses and provide a short explanation. Avoid any position biases and ensure that the\n",
      "order in which the responses were presented does not influence your decision. Do not allow\n",
      "the length of the responses to influence your evaluation. Do not favor certain names of\n",
      "the assistants. Be as objective as possible. After providing your explanation, output your\n",
      "final verdict by strictly following this format: \"[[A]]\" if assistant A is better, \"[[B]]\"\n",
      "if assistant B is better, and \"[[C]]\" for a tie.\n",
      "[User Question]\n",
      "하늘은 왜 하늘색인가요?\n",
      "[The Start of Assistant A’s Answer]\n",
      "하늘의 색깔이 왜 하늘색인지 설명하기 위해서는 대기 중의 빛 산란 현상을 이해해야 합니다. 태양으로부터 오는 빛은 사실 모든 색깔을 포함하고 있는데, 이 빛이 지구 대기에 도달하면 대기 분자들과 상호작용합니다.\n",
      "\n",
      "이 과정에서 '레일리 산란(Rayleigh scattering)'이라는 현상이 발생합니다. 레일리 산란은 대기 분자들이 파장이 짧은 빛(예를 들어, 파란색과 보라색)을 다른 색의 빛보다 더 많이 산란시키는 현상입니다. 파란색 빛의 파장은 상대적으로 짧기 때문에 산란될 때 공기 분자들에 의해 더 효과적으로 퍼져 나갑니다. 이 때문에 우리가 하늘을 보았을 때 대부분 파란색으로 보이는 것입니다.\n",
      "\n",
      "반면에, 해가 지평선에 가까울 때(일출이나 일몰 시) 하늘의 색은 빨강, 주황, 노랑과 같은 따뜻한 색으로 보이는데, 이는 태양 빛이 대기를 통과하는 거리가 더 길어지면서 파란색 빛이 더 많이 산란되고, 상대적으로 파장이 긴 빛(빨간색, 주황색)이 우리 눈에 도달하기 때문입니다. 이러한 현상을 '미에 산란(Mie scattering)'이라고 합니다.\n",
      "\n",
      "따라서 하늘이 파란색으로 보이는 주된 이유는 대기 중의 레일리 산란 때문입니다.\n",
      "[The End of Assistant A’s Answer]\n",
      "[The Start of Assistant B’s Answer]\n",
      "하늘의 색깔이 하늘색, 즉 파란색으로 보이는 이유는 대기 중의 분자들과 작은 입자들이 태양 빛을 산란시키기 때문입니다. 이 현상을 '레일리 산란'이라고 합니다.\n",
      "\n",
      "태양에서 오는 빛은 여러 가지 색의 빛으로 구성되어 있으며, 이 빛들은 파장의 길이에 따라 다르게 산란됩니다. 레일리 산란은 파장이 짧은 빛 (예: 파란색, 보라색)이 파장이 긴 빛 (예: 빨간색, 주황색)보다 더 많이 산란되는 현상입니다. 파란색 빛의 파장은 매우 짧기 때문에 대기 중을 통과할 때 다른 색의 빛보다 훨씬 더 많이 산란되어, 우리 눈에는 하늘이 파란색으로 보이게 됩니다.\n",
      "\n",
      "그러나 해가 지평선에 가까울 때, 즉 일출이나 일몰 때에는 하늘이 빨강, 주황, 노랑과 같은 따뜻한 색으로 보이는데, 이는 태양 빛이 대기를 통과하는 거리가 훨씬 길어져서 파란색과 보라색 빛이 대기에 의해 더 많이 산란되고, 빨간색과 주황색 빛이 상대적으로 덜 산란되어 우리 눈에 도달하기 때문입니다.\n",
      "[The End of Assistant B’s Answer]\n"
     ]
    }
   ],
   "source": [
    "# 논문 프롬프트 참고 (a와 b중 더 좋은 답변을 찾기 위한 프롬프트)\n",
    "prompt = f\"\"\"[System]\n",
    "Please act as an impartial judge and evaluate the quality of the responses provided by two\n",
    "AI assistants to the user question displayed below. You should choose the assistant that\n",
    "follows the user’s instructions and answers the user’s question better. Your evaluation\n",
    "should consider factors such as the helpfulness, relevance, accuracy, depth, creativity,\n",
    "and level of detail of their responses. Begin your evaluation by comparing the two\n",
    "responses and provide a short explanation. Avoid any position biases and ensure that the\n",
    "order in which the responses were presented does not influence your decision. Do not allow\n",
    "the length of the responses to influence your evaluation. Do not favor certain names of\n",
    "the assistants. Be as objective as possible. After providing your explanation, output your\n",
    "final verdict by strictly following this format: \"[[A]]\" if assistant A is better, \"[[B]]\"\n",
    "if assistant B is better, and \"[[C]]\" for a tie.\n",
    "[User Question]\n",
    "{question}\n",
    "[The Start of Assistant A’s Answer]\n",
    "{answer_a}\n",
    "[The End of Assistant A’s Answer]\n",
    "[The Start of Assistant B’s Answer]\n",
    "{answer_b}\n",
    "[The End of Assistant B’s Answer]\"\"\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a874f218-0f4d-4000-982c-7d205f8b5c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both Assistant A and Assistant B provide accurate explanations for why the sky appears blue, focusing on the concept of Rayleigh scattering. They both mention that shorter wavelengths of light, such as blue, are scattered more than longer wavelengths, which is why the sky appears blue during the day. They also both explain why the sky appears red or orange during sunrise and sunset, due to the longer path through the atmosphere and the scattering of shorter wavelengths.\n",
      "\n",
      "However, Assistant A provides a slightly more detailed explanation by mentioning \"Mie scattering\" in the context of the sky's color during sunrise and sunset, although this is not entirely accurate as Mie scattering is not the primary reason for the color change at these times. Assistant A also provides a more structured explanation, starting with the interaction of sunlight with atmospheric molecules and then explaining the scattering phenomena.\n",
      "\n",
      "Assistant B's response is concise and accurate but lacks the additional detail about Mie scattering, which, despite being slightly misplaced, adds depth to Assistant A's explanation.\n",
      "\n",
      "Overall, both responses are quite similar in quality, but Assistant A's response is marginally more detailed.\n",
      "\n",
      "[[A]]\n"
     ]
    }
   ],
   "source": [
    "# 가장 최신 gpt-4o 모델로 앞선 두 모델의 응답을 평가\n",
    "completion = client.chat.completions.create(model='gpt-4o',\n",
    "                                            messages = [{'role' : 'user',\n",
    "                                                         'content' : prompt}],\n",
    "                                            temperature = 0,\n",
    "                                           )\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3812ba8b-e2f6-4014-95f7-61fe53f33dca",
   "metadata": {},
   "source": [
    "### 장단점 비교\n",
    "1. Human Based Evaluation\n",
    "- 통제된 환경을 가정했을 때 사람이 직접 평가한 방법이라 안정적이고 신뢰할 수 있음\n",
    "- 전문가가 아닌 불특정 다수의 경우 약간의 노이즈가 발생할 수 있음\n",
    "- 전문가가 아닌 경우 평가 정확도 저하 및 속도가 더 오래걸릴 수 있음\n",
    "\n",
    "2. Model Based Evaluation\n",
    "- 사람 평가와 어느정도 유사한 수준의 평가를 내릴 수 있음\n",
    "- 평가를 위해 API호출이 필요한데 평가 데이터가 굉장히 많을 경우 수백만원 이상 소요됨\n",
    "\n",
    "3. Code Based Evaluation\n",
    "- 위 방법들에 비해 인력 비용, 모델 호출 비용 등이 없는 무료 평가 방법\n",
    "- task에 따라 활용할 수 있는 범위가 제한적\n",
    "- 사람들에게 적합한 답변을 선택하는데 있어서는 신뢰도가 상대적으로 떨어지는 편"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103ef809-cdd7-49dd-991f-7e81bbbcb9ca",
   "metadata": {},
   "source": [
    "### 프롬프트 엔지니어링 고급 기법 적용\n",
    "1. **few-shot**\n",
    "- 참고할 수 있는 문제-정답 예시나 사례들을 프롬프트에 추가하여 질의\n",
    "- 논문 : https://arxiv.org/abs/2005.14165\n",
    "2. **Chain-of-thought(CoT)**\n",
    "- Few-shot에 추가로 문제 해결과정을 단계별로 모델에게 알려주면서 질의\n",
    "- 논문 : https://arxiv.org/abs/2201.11903"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0937c281-8675-4f56-9070-f65f5bf08e30",
   "metadata": {},
   "source": [
    "#### Zero-show\n",
    "- 질의에 아무런 예시가 없는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dd820bc8-b3b0-47f7-b7a4-1b5093294ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J.K. Rowling wrote the book 'Harry Potter'.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Q: Who wrote the book 'HARRY POTTER'?\"\"\"\n",
    "\n",
    "completion = client.chat.completions.create(model = 'gpt-3.5-turbo',\n",
    "                                            messages = [{'role' : 'user',\n",
    "                                                         'content' : prompt}],\n",
    "                                            temperature = 0\n",
    "                                           )\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777e4c94-405c-47ce-9274-2c4595b31458",
   "metadata": {},
   "source": [
    "#### Few-show\n",
    "- few-show(One-shot)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1adc2168-68c3-46cb-a737-61cd7020748a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: J.K. Rowling\n"
     ]
    }
   ],
   "source": [
    "# 예시를 명시해주니 이름만 말함\n",
    "prompt = \"\"\" Answer these question:\n",
    "Q: Who wrote the book 'HARRY POTTER'?\n",
    "\n",
    "Below is an example for your reference.\n",
    "Q: Who sang 'One Call Away'?\n",
    "A: Charlie Puth\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "completion = client.chat.completions.create(model = 'gpt-3.5-turbo',\n",
    "                                            messages = [{'role' : 'user',\n",
    "                                                         'content' : prompt}],\n",
    "                                            temperature = 0\n",
    "                                           )\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e864202f-43af-4c38-af57-633aecb7a63c",
   "metadata": {},
   "source": [
    "#### Chain of Thought\n",
    "- 마이크로 소프트에서 사용한 예시 프롬프트 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "edc83467-93d8-4bbc-aa0d-f40b2e923a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice would have 5 apples - 3 thrown + 2 given to Bob + 1 given back by Bob = 5 apples. \n",
      "\n",
      "So, Alice still has 5 apples.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Alice has 5 apples, throws 3 apples, gives 2 to Bob and Bob gives one back, \\n\n",
    "how many apples does Alice have?\"\"\"\n",
    "\n",
    "completion = client.chat.completions.create(model = 'gpt-3.5-turbo',\n",
    "                                            messages = [{'role' : 'user',\n",
    "                                                         'content' : prompt}],\n",
    "                                            temperature = 0\n",
    "                                           )\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c486797-1c37-4fb1-b6e4-db4e0158c821",
   "metadata": {},
   "source": [
    "#### CoT + One-shot\n",
    "- Below ~ : One-shot\n",
    "- 계산식 : CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e163a6c4-ab1e-4eaf-8812-4f6773043a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice has 5 apples - 3 thrown = 2 apples\n",
      "2 apples - 2 given to Bob = 0 apples\n",
      "0 apples + 1 from Bob = 1 apple\n",
      "\n",
      "Therefore, Alice has 1 apple.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Alice has 5 apples, throws 3 apples, gives 2 to Bob and Bob gives one back, \\n\n",
    "how many apples does Alice have?\n",
    "\n",
    "    Below is an example for your reference.\n",
    "\n",
    "    Lisa has 7 apples, throws 1 apple, gives 4 apples to Bart and Bart gives one back:\n",
    "    7 - 1 = 6\n",
    "    6 - 4 = 2\n",
    "    2 + 1 = 3\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "completion = client.chat.completions.create(model = 'gpt-3.5-turbo',\n",
    "                                            messages = [{'role' : 'user',\n",
    "                                                         'content' : prompt}],\n",
    "                                            temperature = 0\n",
    "                                           )\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0600ad-3fd8-4a13-b096-f1f544529b66",
   "metadata": {},
   "source": [
    "#### 또 다른 프롬프트 고도화 예시\n",
    "- KMMLU(Measuring Massive Multitask Language Understanding in Korea) 논문의 프롬프트 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "79d8a7a8-5e2b-429b-be9f-599665831697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실제 정답은 B=6\n",
    "question = 'x, y가 세 부등식 y ≤ x+3, y ≤ -4x+3, y ≥ 0을 만족할 때, x+y의 최댓값을 M, 최솟값을 m이라 하면 M-m의 값은?'\n",
    "A = 4\n",
    "B = 6\n",
    "C = 8\n",
    "D = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b3d45a1c-aaee-45b5-9447-1cd4b86b3fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"{question}\n",
    "A. {A}\n",
    "B. {B}\n",
    "C. {C}\n",
    "D. {D}\n",
    "정답 : \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "39e46b15-1d82-4f35-b036-c99d0b8bce9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C. 8\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(model = 'gpt-3.5-turbo',\n",
    "                                            messages = [{'role' : 'user',\n",
    "                                                         'content' : prompt}],\n",
    "                                            temperature = 0\n",
    "                                           )\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9f384b-39f7-4b2c-ae27-76315b418084",
   "metadata": {},
   "source": [
    "#### 3.5모델 고도화 시키기\n",
    "- 페르소나 적용\n",
    "- 영문 prompt 작성\n",
    "- 효과적인 prompt 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "90d0a0b4-79b5-478b-b3f0-718327f15fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C. 8\n"
     ]
    }
   ],
   "source": [
    "# 페르소나 적용\n",
    "prompt = f\"\"\"You are an Professinal in Mathematics.\n",
    "Bellow is given a math question in Korea.\n",
    "\n",
    "{question}\n",
    "A. {A}\n",
    "B. {B}\n",
    "C. {C}\n",
    "D. {D}\n",
    "정답 : \n",
    "\"\"\"\n",
    "\n",
    "completion = client.chat.completions.create(model = 'gpt-3.5-turbo',\n",
    "                                            messages = [{'role' : 'user',\n",
    "                                                         'content' : prompt}],\n",
    "                                            temperature = 0\n",
    "                                           )\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "9ab5b662-76e8-4368-8d2d-8aa9ea8df8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C. 8\n",
      "\n",
      "우선 주어진 부등식을 그래프로 그려보겠습니다.\n",
      "\n",
      "1. y ≤ x+3\n",
      "이는 y = x+3인 직선을 포함하며, (0,3)을 지나고 기울기가 1인 직선입니다.\n",
      "\n",
      "2. y ≤ -4x+3\n",
      "이는 y = -4x+3인 직선을 포함하며, (0,3)을 지나고 기울기가 -4인 직선입니다.\n",
      "\n",
      "3. y ≥ 0\n",
      "이는 x축 위의 점들을 포함하는 영역입니다.\n",
      "\n",
      "이 세 부등식을 만족하는 영역은 y ≤ x+3, y ≤ -4x+3, y ≥ 0이 교차하는 부분으로, 삼각형 모양의 영역입니다.\n",
      "\n",
      "이제 x+y의 최댓값을 구하기 위해, 삼각형의 꼭지점을 찾아야 합니다. 이 꼭지점은 세 부등식이 만나는 지점 중에서 가장 큰 값을 가지게 됩니다.\n",
      "\n",
      "y = x+3과 y = -4x+3을 동시에 만족하는 지점을 찾으면, x = 0, y = 3이 됩니다. 따라서 이 꼭지점은 (0,3)입니다.\n",
      "\n",
      "따라서 x+y의 최댓값 M은 3+0 = 3이 됩니다.\n",
      "\n",
      "이제 x+y의 최솟값을 구하기 위해, 삼각형의 다른 꼭지점을 찾아야 합니다. 이 꼭지점은 세 부등식이 만나는 지점 중에서 가장 작은 값을 가지게 됩니다.\n",
      "\n",
      "y = x+3과 y = 0을 동시에 만족하는 지점을 찾으면, x = -3, y = 0이 됩니다. 따라서 이 꼭지점은 (-3,0)입니다.\n",
      "\n",
      "따라서 x+y의 최솟값 m은 0+(-3) = -3이 됩니다.\n",
      "\n",
      "따라서 M-m = 3 - (-3) = 6이므로, 정답은 8이 됩니다.\n"
     ]
    }
   ],
   "source": [
    "# 페르소나 적용\n",
    "prompt = f\"\"\"You are an Professinal in Mathematics.\n",
    "Bellow is given a math question in Korea.\n",
    "You have to think carefully and step by step about the question and choose one of four given answers.\n",
    "Only one of them is ture. and explain it in Korea.\n",
    "After calculating, check the result again.\n",
    "\n",
    "Give resason step by step and carefully about why you think your answer is correct.\n",
    "\n",
    "\n",
    "{question}\n",
    "A. {A}\n",
    "B. {B}\n",
    "C. {C}\n",
    "D. {D}\n",
    "정답 : \n",
    "\"\"\"\n",
    "\n",
    "completion = client.chat.completions.create(model = 'gpt-3.5-turbo',\n",
    "                                            messages = [{'role' : 'user',\n",
    "                                                         'content' : prompt}],\n",
    "                                            temperature = 0\n",
    "                                           )\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769138f4-855f-47df-8ca4-98c354fcbe81",
   "metadata": {},
   "source": [
    "### 프롬프트 엔지니어링 특징 정리\n",
    "- 추가 학습이 없음에도 성능 개선의 가능성이 있기 때문에 가성비가 굉장히 좋음\n",
    "- 더 좋은 모델을 사용하면 프롬프트 엔지니어링 없이도 해결할 수 있지만 비용 측면을 무시할 수 없기 때문에 먼저 프롬프트 엔지니어링으로 성능향상을 시도해보는 것이 좋음(특히 현업에서는 비용 문제에 굉장히 민감함)\n",
    "- 단 모델의 알고리즘이 완전히 바뀌어 버리면 기존에 사용했던 방법들이 크게 달라질 수 있어서 변동성이 높은 편\n",
    "- 프롬프트 엔지니어링으로도 해결이 되지 않으면 RAG나 Finetuning으로 성능을 향상시킬 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0e1716-817a-4304-bb66-73c7544435d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:study]",
   "language": "python",
   "name": "conda-env-study-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
