{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tc1GLOlzNTp2"
      },
      "source": [
        "# 텍스트 전처리 (Text Preprocessing)\n",
        "*   텍스트를 자연어 처리를 위해 용도에 맞도록 사전에 표준화 하는 작업\n",
        "*   텍스트 내 정보를 유지하고, 중복을 제거하여 분석 효율성을 높이기 위해 전처리를 수행\n",
        "\n",
        "### 1) 토큰화 (Tokenizing)\n",
        "* 토큰화는 단어별로 분리하는 \"단어 토큰화(Word Tokenization)\"와 문장별로 분리하는 \"문장 토큰화(Sentence Tokenization)\"로 구분\n",
        "(이후 실습에서는 단어 토큰화를 \"토큰화\"로 통일)\n",
        "\n",
        "### 2) 품사 부착(PoS Tagging)\n",
        "* 각 토큰에 품사 정보를 추가\n",
        "* 분석시에 불필요한 품사를 제거하거나 (예. 조사, 접속사 등) 필요한 품사를 필터링 하기 위해 사용\n",
        "\n",
        "### 3) 개체명 인식 (NER, Named Entity Recognition)\n",
        "* 각 토큰의 개체 구분(기관, 인물, 지역, 날짜 등) 태그를 부착\n",
        "* 텍스트가 무엇과 관련되어있는지 구분하기 위해 사용\n",
        "* 예를 들어, 과일의 apple과 기업의 apple을 구분하는 방법이 개체명 인식임\n",
        "\n",
        "### 4) 원형 복원 (Stemming & Lemmatization)\n",
        "* 각 토큰의 원형 복원을 함으로써 토큰을 표준화하여 불필요한 데이터 중복을 방지 (=단어의 수를 줄일수 있어 연산을 효율성을 높임)\n",
        "* 어간 추출(Stemming) : 품사를 무시하고 규칙에 기반하여 어간을 추출 - 규칙기반\n",
        "* 표제어 추출 (Lemmatization) : 품사정보를 유지하여 표제어 추출 - 사전기반\n",
        "\n",
        "### 5) 불용어 처리 (Stopword)\n",
        "* 자연어 처리를 위해 불필요한 요소를 제거하는 작업\n",
        "* 불필요한 품사를 제거하는 작업과 불필요한 단어를 제거하는 작업으로 구성\n",
        "* 불필요한 토큰을 제거함으로써 연산의 효율성을 높임"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aNa59M5NewL"
      },
      "source": [
        "# 1 영문 전처리 실습\n",
        "\n",
        "- NLTK 토크나이저 사용\n",
        "  - 교육용으로 개발된 자연어 처리 및 문서 분석용 파이썬 패키지\n",
        "\n",
        "- [NLTK](https://www.nltk.org/) lib 사용\n",
        "- [영문 토큰화](https://www.nltk.org/api/nltk.tokenize.html) 모듈\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-weHtd3WkyZH",
        "outputId": "466863a1-63b1-4f15-ee59-e3e46ebda49c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akTwqILNeQEA"
      },
      "source": [
        "### 1.1. word_tokenize()\n",
        "- 마침표와 구두점(온점(.), 컴마(,), 물음표(?), 세미콜론(;), 느낌표(!) 등과 같은 기호)으로 구분하여 토큰화\n",
        "- 내부적으로 TreebankWordTokenizer를 사용하기 때문에 결과가 동일한 경우가 많음\n",
        "- NLTK에서 제공하는 고수준 함수이며 더 나은 추상화와 호환성 제공\n",
        "\n",
        "### nltk.download('punkt')\n",
        "- punkt는 언어 독립적인 문장 분할기를 포함하고 있어, 텍스트를 문장 단위로 나누는 데 사용.\n",
        "- 예를 들어, 문단 내에서 개별 문장을 분리하거나, 문장 끝을 인식하는 작업에 활용됩니다.\n",
        "- Punkt 토크나이저는 규칙 기반이 아닌 비지도 학습 방법으로 학습되어 있어 다양한 언어와 문장 구조에 잘 적용될 수 있음."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgOMYw1Lkhse",
        "outputId": "4662bbb6-4a35-409f-82dc-5bc02d113624"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['Hello',\n",
              " '!',\n",
              " 'I',\n",
              " 'ca',\n",
              " \"n't\",\n",
              " 'wait',\n",
              " 'to',\n",
              " 'try',\n",
              " 'the',\n",
              " 'word_tokenize',\n",
              " ',',\n",
              " 'WordPunctTokenizer',\n",
              " ',',\n",
              " 'and',\n",
              " 'TreebankWordTokenizer',\n",
              " '.']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt_tab')\n",
        "text = \"Hello! I can't wait to try the word_tokenize, WordPunctTokenizer, and TreebankWordTokenizer.\"\n",
        "word_token = word_tokenize(text)\n",
        "word_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tksL4pyhbA_y",
        "outputId": "20edab99-454b-4168-ae19-14ab7e81353f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['They', \"'ll\", 'save', 'and', 're-use', 'this', 'file', '.']"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# word_tokenize\n",
        "text = \"They'll save and re-use this file.\"\n",
        "word_tokens = word_tokenize(text)\n",
        "word_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7chqlQZei8O"
      },
      "source": [
        "### WordPunctTokenizer()  \n",
        "- 알파벳이 아닌 문자를 구분하여 토큰화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fnfnsRvrk1bJ",
        "outputId": "d9421cdd-c399-499f-f430-97962e24b0cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Hello',\n",
              " '!',\n",
              " 'I',\n",
              " 'can',\n",
              " \"'\",\n",
              " 't',\n",
              " 'wait',\n",
              " 'to',\n",
              " 'try',\n",
              " 'the',\n",
              " 'word_tokenize',\n",
              " ',',\n",
              " 'WordPunctTokenizer',\n",
              " ',',\n",
              " 'and',\n",
              " 'TreebankWordTokenizer',\n",
              " '.']"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "text = \"Hello! I can't wait to try the word_tokenize, WordPunctTokenizer, and TreebankWordTokenizer.\"\n",
        "wordpunct = WordPunctTokenizer().tokenize(text)\n",
        "wordpunct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOWNhGV6bC9n",
        "outputId": "bf30cf30-e45c-43ec-9bf6-b06f9e15824f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['They', \"'\", 'll', 'save', 'and', 're', '-', 'use', 'this', 'file', '.']"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# WordPunctTokenizer\n",
        "text = \"They'll save and re-use this file.\"\n",
        "wordpunct = WordPunctTokenizer().tokenize(text)\n",
        "wordpunct"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_S9M6q0ofJ_c"
      },
      "source": [
        "### TreebankWordTokenizer()\n",
        "- Penn Treebank에서 사용하는 규칙에 따라 토큰 분리\n",
        "- 구두점을 분리하고 축약형에서 \"n't\"를 분리하는 특징이 있음\n",
        "- 실제 토큰화 작업을 수행하는 저수준 클래스"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Znl58b4ZeLAw",
        "outputId": "d2b7acf3-5207-40c9-89d5-3a6a16244bd4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Hello',\n",
              " '!',\n",
              " 'I',\n",
              " 'ca',\n",
              " \"n't\",\n",
              " 'wait',\n",
              " 'to',\n",
              " 'try',\n",
              " 'the',\n",
              " 'word_tokenize',\n",
              " ',',\n",
              " 'WordPunctTokenizer',\n",
              " ',',\n",
              " 'and',\n",
              " 'TreebankWordTokenizer',\n",
              " '.']"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "text = \"Hello! I can't wait to try the word_tokenize, WordPunctTokenizer, and TreebankWordTokenizer.\"\n",
        "treebankwordtoken = TreebankWordTokenizer().tokenize(text)\n",
        "treebankwordtoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBApnvfBbwnC",
        "outputId": "01636989-6c38-4edb-d85f-c2c49346ff1f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['They', \"'ll\", 'save', 'and', 're-use', 'this', 'file', '.']"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = \"They'll save and re-use this file.\"\n",
        "treebankwordtoken = TreebankWordTokenizer().tokenize(text)\n",
        "treebankwordtoken"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyOxN9pLlFGD"
      },
      "source": [
        "## 1.2. 영문 품사 부착 (PoS Tagging)\n",
        "분리한 토큰마다 품사를 부착한다\n",
        "\n",
        "- [nltk.tag](https://www.nltk.org/api/nltk.tag.html) 패키지\n",
        "\n",
        "- [태크목록](https://pythonprogramming.net/natural-language-toolkit-nltk-part-speech-tagging/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsSXVogPigSS"
      },
      "source": [
        "### nltk.download('averaged_perceptron_tagger')\n",
        "- averaged_perceptron_tagger는 POS 태깅을 수행하는 데 사용되는 학습된 모델.\n",
        "- 이 모델은 비지도 학습 방식으로 텍스트 데이터를 기반으로 품사 태그를 학습하고, 이후 새로운 텍스트에서 품사를 예측할 수 있음."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5zeqK6LfJe4",
        "outputId": "6fe1c58c-4d60-4a41-d95c-d3e42694fbca"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWx7Y7RrpdlP"
      },
      "source": [
        "### 주요 POS 태그 목록\n",
        "- [POS 태그 목록 확인](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fL5t1GzflkTq",
        "outputId": "31af1ecf-f5c1-4ca4-bd1b-5213975cb184"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('They', 'PRP'),\n",
              " (\"'ll\", 'MD'),\n",
              " ('save', 'VB'),\n",
              " ('and', 'CC'),\n",
              " ('re-use', 'VB'),\n",
              " ('this', 'DT'),\n",
              " ('file', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# pos 태그\n",
        "taggedToken = pos_tag(word_tokens)\n",
        "taggedToken"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJU2siKllpIC"
      },
      "source": [
        "## 1.3. 개체명 인식 (NER, Named Entity Recognition)\n",
        "- 개체명 인식은 고유명사(예: 인물\n",
        "[nltk.chunk](http://www.nltk.org/api/nltk.chunk.html)패키지"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJEzv405jIz7"
      },
      "source": [
        "### nltk.download('words')\n",
        "- 이 데이터셋은 NLTK에서 텍스트 처리 시 올바른 영어 단어를 인식하거나 철자 오류를 교정하는 데 사용될 수 있음.\n",
        "- 이 데이터셋은 개체명 인식(NE chunking)과 같은 작업에서 단어의 존재 여부를 확인하는 데에도 사용.\n",
        "\n",
        "### nltk.download('maxent_ne_chunker')\n",
        "- maxent_ne_chunker는 최대 엔트로피(Maximum Entropy) 모델을 사용한 개체명 인식(NER) 시스템.\n",
        "- 이 모델은 텍스트에서 사람 이름, 장소, 조직 등의 고유명사를 인식하고, 이를 적절한 범주로 분류.\n",
        "- maxent_ne_chunker는 NLTK에서 제공하는 사전 학습된 NER 모델 중 하나로, 주어진 텍스트에서 특정 엔티티를 식별하고 라벨링하는 데 사용."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41_6F6JYllr-",
        "outputId": "6b1ecd7d-f6de-40bb-c6c4-7ffef47d7b07"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk import ne_chunk\n",
        "nltk.download('maxent_ne_chunker_tab')\n",
        "nltk.download('words')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJtcy-Z7cDQv",
        "outputId": "895dd9e3-9842-482e-c302-caaa2147b868"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('They', 'PRP'), (\"'ll\", 'MD'), ('save', 'VB'), ('and', 'CC'), ('re-use', 'VB'), ('this', 'DT'), ('file', 'NN'), ('.', '.')]\n",
            "(S They/PRP 'll/MD save/VB and/CC re-use/VB this/DT file/NN ./.)\n"
          ]
        }
      ],
      "source": [
        "# pos태깅된 taggedToken으로 NER\n",
        "print(taggedToken)\n",
        "neToken = ne_chunk(taggedToken)\n",
        "print(neToken)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-xVXtwxensb"
      },
      "source": [
        "## 1.4. 원형 복원\n",
        "- 각 토큰의 원형을 복원하여 표준화 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_u6DO-IesLm"
      },
      "source": [
        "### 1.4.1. 어간추출 (Stemming)\n",
        "\n",
        "- 규칙에 기반 하여 토큰을 표준화\n",
        "- ning제거, ful 제거 등\n",
        "\n",
        "- [nltk.stem](https://www.nltk.org/api/nltk.stem.html) 패키지\n",
        "\n",
        "- [규칙상세](https://tartarus.org/martin/PorterStemmer/def.txt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "QEDjB0YHlxuY"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFnCHve5cuPs",
        "outputId": "09610aeb-3ec0-46a8-9fe9-ec1a5a7b12c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "running->run\n",
            "beautiful ->beauti\n",
            "believes ->believ\n",
            "using ->use\n",
            "conversation ->convers\n",
            "organization ->organ\n",
            "studies ->studi\n"
          ]
        }
      ],
      "source": [
        "# running, beautiful, believes, using, conversation, organization, studies 원형 복원\n",
        "print('running->' + ps.stem('running'))\n",
        "print('beautiful ->' + ps.stem('beautiful'))\n",
        "print('believes ->' + ps.stem('believes'))\n",
        "print('using ->' + ps.stem('using'))\n",
        "print('conversation ->' + ps.stem('conversation'))\n",
        "print('organization ->' + ps.stem('organization'))\n",
        "print('studies ->' + ps.stem('studies'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oAW1rmlf8Nf"
      },
      "source": [
        "### 1.4.2. 표제어 추출 (Lemmatization)\n",
        "\n",
        "- 품사정보를 보존하여 토큰을 표준화\n",
        "\n",
        "- [nlt.stem](http://www.nltk.org/api/nltk.stem.html?highlight=lemmatizer) 패키지\n",
        "\n",
        "##nltk.download('wordnet')\n",
        "- WordNet은 영어 어휘 데이터베이스로, 단어들의 의미와 그들 간의 관계를 정리한 큰 사전이다.\n",
        "- 동의어 집합(synsets), 반의어(antonyms), 상위어(hypernyms), 하위어(hyponyms) 등의 의미적 관계를 포함합니다.\n",
        "- WordNet은 NLP 작업에서 단어의 의미를 이해하고 단어 간의 관계를 분석하는 데 유용합니다.\n",
        "- 예를 들어, \"dog\"이라는 단어의 동의어와 그와 관련된 개념들을 WordNet을 통해 찾을 수 있습니다.\n",
        "\n",
        "##nltk.download('omw-1.4')\n",
        "- OMW는 WordNet의 다국어 버전으로, 여러 언어에 걸쳐 WordNet의 개념을 제공함.\n",
        "- OMW-1.4는 이 다국어 데이터베이스의 버전 중 하나임.\n",
        "- 이를 통해 영어 외의 다른 언어에 대해 WordNet과 같은 방식으로 어휘적 및 의미적 분석을 수행할 수 있음.\n",
        "- 예를 들어, 영어 단어를 다른 언어로 번역하거나, 다른 언어의 단어와 관련된 의미적 정보를 찾을 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eKJ8302f6B5",
        "outputId": "302aaad1-0a7c-4b31-d151-b4cdb8f7d73c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "9wYoA28igang"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# WordNetLemmatizer 객체 생성\n",
        "wl = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TivON898dWC-"
      },
      "source": [
        "- 동사는 wl.lemmatize(text,pos=wordnet.VERB)\n",
        "- 형용사는 wl.lemmatize(text, pos=wordnet.ADJ)\n",
        "- 명사는 wl.lemmatize(text, pos=wordnet.NOUN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOozg6PldDtY",
        "outputId": "b347e4e7-5a99-4296-c6a8-e4012fbbffdf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "동사형태 : running ->run\n",
            "형용사형태 : beautiful ->beautiful\n",
            "명사형태 : geese ->goose\n"
          ]
        }
      ],
      "source": [
        "# 품사를 명시하여 lemmatize 적용(각 품사마다 리스트컴프리헨션으로 돌려서 변경해줘야함)\n",
        "# 동사 한번 돌리고, 형용사 한 번 돌리고, 명사 한 번 돌리고\n",
        "print('동사형태 : running ->' + wl.lemmatize('running', pos=wordnet.VERB))\n",
        "print('형용사형태 : beautiful ->' + wl.lemmatize('beautiful', pos=wordnet.ADJ))\n",
        "print('명사형태 : geese ->' + wl.lemmatize('geese', pos=wordnet.NOUN))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kIfr8K0kpgK"
      },
      "source": [
        "## 1.5. 불용어 처리 (Stopword)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNfCRHo2n6li"
      },
      "source": [
        "### 단어들로 불용어 처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ZWrWJ3eLn9UP"
      },
      "outputs": [],
      "source": [
        "# 불용어 리스트 (불용어로 간주할 단어들)\n",
        "stopwords = ['the', 'is', 'in', 'and', 'to', 'a', 'of']\n",
        "text = \"The quick brown fox jumps over the lazy dog. The dog barked loudly at the fox in the park.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "d0_uXCmodqYN"
      },
      "outputs": [],
      "source": [
        "# 텍스트를 단어 단위로 토큰화\n",
        "tokens = word_tokenize(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "0b2ykgE6drbV"
      },
      "outputs": [],
      "source": [
        "# 불용어 제거\n",
        "filtered_tokens  = [word for word in tokens if word.lower() not in stopwords]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JaW8HHkBdtB3",
        "outputId": "2e731c06-bb47-4568-866c-86f0aac723b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "원래 토큰: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'The', 'dog', 'barked', 'loudly', 'at', 'the', 'fox', 'in', 'the', 'park', '.']\n",
            "불용어 제거 후 토큰: ['quick', 'brown', 'fox', 'jumps', 'over', 'lazy', 'dog', '.', 'dog', 'barked', 'loudly', 'at', 'fox', 'park', '.']\n"
          ]
        }
      ],
      "source": [
        "# 결과 출력\n",
        "print(\"원래 토큰:\", tokens)\n",
        "print(\"불용어 제거 후 토큰:\", filtered_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRCx3X9ynprx"
      },
      "source": [
        "### POS 태깅으로 불용어 처리\n",
        "- IN: 전치사 또는 종속 접속사\n",
        "- CC: 등위 접속사\n",
        "- UH: 감탄사\n",
        "- TO: 전치사 \"to\"\n",
        "- MD: 조동사\n",
        "- DT: 한정사\n",
        "- VBZ: 동사, 3인칭 단수 현재형\n",
        "- VBP: 동사, 비 3인칭 단수 현재형"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "uqyDVQcggcKl"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "# nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# 불용어 품사 정의\n",
        "stopPos = ['IN', 'CC', 'UH', 'TO', 'MD', 'DT', 'VBZ','VBP']\n",
        "\n",
        "text = \"The quick brown fox jumps over the lazy dog. \\\n",
        "        The dog barked loudly at the fox.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10ktEYEkd1Oz",
        "outputId": "d6aeb310-406a-4a95-9864-0915b671eeb9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['The',\n",
              " 'quick',\n",
              " 'brown',\n",
              " 'fox',\n",
              " 'jumps',\n",
              " 'over',\n",
              " 'the',\n",
              " 'lazy',\n",
              " 'dog',\n",
              " '.',\n",
              " 'The',\n",
              " 'dog',\n",
              " 'barked',\n",
              " 'loudly',\n",
              " 'at',\n",
              " 'the',\n",
              " 'fox',\n",
              " '.']"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 텍스트를 단어 단위로 토큰화\n",
        "tokens = word_tokenize(text)\n",
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Fvz5yAed2Uw",
        "outputId": "63e32486-84ba-43e1-809e-6bb19163020d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('They', 'PRP'),\n",
              " (\"'ll\", 'MD'),\n",
              " ('save', 'VB'),\n",
              " ('and', 'CC'),\n",
              " ('re-use', 'VB'),\n",
              " ('this', 'DT'),\n",
              " ('file', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 각 단어에 대해 품사 태깅 수행\n",
        "taggedTokken = nltk.pos_tag(tokens)\n",
        "taggedToken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVNStofSd42Y",
        "outputId": "8dff816e-0043-4aad-ef2f-c47460972c07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(('They', 'PRP'), 1), ((\"'ll\", 'MD'), 1), (('save', 'VB'), 1), (('and', 'CC'), 1), (('re-use', 'VB'), 1), (('this', 'DT'), 1), (('file', 'NN'), 1), (('.', '.'), 1)]\n"
          ]
        }
      ],
      "source": [
        "# 최빈어 조회\n",
        "print(Counter(taggedToken).most_common())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "weqC8skqd59W"
      },
      "outputs": [],
      "source": [
        "# 불용어 처리: 특정 품사 태그에 해당하는 단어만 필터링\n",
        "stopwords = [word[0] for word in taggedToken if word[1] in stopPos]\n",
        "filtered_tokens = [word[0] for word in taggedToken if word[1] not in stopwords]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJx7rZCRd67O",
        "outputId": "a69a497c-e4eb-4f56-991b-649aac0a696c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "품사 태깅 결과 :  [('They', 'PRP'), (\"'ll\", 'MD'), ('save', 'VB'), ('and', 'CC'), ('re-use', 'VB'), ('this', 'DT'), ('file', 'NN'), ('.', '.')]\n",
            "불용어로 간주된 단어들 :  [\"'ll\", 'and', 'this']\n",
            "불용어가 삭제된 토큰들 :  ['They', \"'ll\", 'save', 'and', 're-use', 'this', 'file', '.']\n"
          ]
        }
      ],
      "source": [
        "# 결과 출력\n",
        "print(\"품사 태깅 결과 : \", taggedToken)\n",
        "print(\"불용어로 간주된 단어들 : \", stopwords)\n",
        "print(\"불용어가 삭제된 토큰들 : \", filtered_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKJTUJMzpvKk"
      },
      "source": [
        "# 2 한글 전처리 실습\n",
        "한글의 경우 한글에 알맞는 토크나이저를 사용하여야 함."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqO3pCeLrvZ3"
      },
      "source": [
        "## 2.1. 한글 토큰화 및 형태소 분석\n",
        "## konlpy\n",
        "- KoNLPy(코엔엘파이)는 파이썬에서 한국어 자연어 처리를 위한 라이브러리이다.\n",
        "- 한국어 텍스트를 분석하고 처리하는 데 필요한 다양한 도구와 기능을 제공한다.\n",
        "- KoNLPy는 형태소 분석, 품사 태깅, 단어 토크나이징, 구문 분석 등을 수행할 수 있으며, 여러 형태소 분석기(예: Hannanum, Kkma, Komoran, Mecab, Okt 등)를 지원하고 있음.\n",
        "- [토크나이저별 성능/시간 비교](https://konlpy-ko.readthedocs.io/ko/v0.4.3/morph/#pos-tagging-with-konlpy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kDMd5DymwhC",
        "outputId": "04899975-4c3c-4ee3-8661-21422ff1453b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading jpype1-1.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.11/dist-packages (from konlpy) (5.3.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.11/dist-packages (from konlpy) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from JPype1>=0.7.0->konlpy) (24.2)\n",
            "Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jpype1-1.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (493 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.9/493.9 kB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.5.1 konlpy-0.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install konlpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2COYZQGdedxb"
      },
      "source": [
        "- 코모란 토큰화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "wteAt8MqpuRI"
      },
      "outputs": [],
      "source": [
        "# 코모란(Komoran) 토큰화\n",
        "from konlpy.tag import Komoran\n",
        "komoran= Komoran()\n",
        "kor_text = \"인간이 컴퓨터와 대화하고 있다는 것을 깨닫지 못하고 인간과 대화를 계속할 수 있다면 컴퓨터는 지능적인 것으로 간주될 수 있습니다.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7aCUNg1elyj",
        "outputId": "270c8294-858f-479b-a281-ada9761cd8a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['인간', '이', '컴퓨터', '와', '대화', '하', '고', '있', '다는', '것', '을', '깨닫', '지', '못하', '고', '인간', '과', '대화', '를', '계속', '하', 'ㄹ', '수', '있', '다면', '컴퓨터', '는', '지능', '적', '이', 'ㄴ', '것', '으로', '간주', '되', 'ㄹ', '수', '있', '습니다', '.']\n"
          ]
        }
      ],
      "source": [
        "komoran_tokens = komoran.morphs(kor_text)\n",
        "print(komoran_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sz9lFY8cefpV"
      },
      "source": [
        "- 한나눔 토큰화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "Ojh61umIsLvW"
      },
      "outputs": [],
      "source": [
        "# 한나눔(Hannanum) 토큰화\n",
        "from konlpy.tag import Hannanum\n",
        "hannanum= Hannanum()\n",
        "kor_text = \"인간이 컴퓨터와 대화하고 있다는 것을 깨닫지 못하고 인간과 대화를 계속할 수 있다면 컴퓨터는 지능적인 것으로 간주될 수 있습니다.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9v9kfaB5eo91",
        "outputId": "3286d431-f784-434f-ea39-d0e0050a04c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['인간', '이', '컴퓨터', '와', '대화', '하고', '있', '다는', '것', '을', '깨닫', '지', '못하', '고', '인간', '과', '대화', '를', '계속', '하', 'ㄹ', '수', '있', '다면', '컴퓨터', '는', '지능적', '이', 'ㄴ', '것', '으로', '간주', '되', 'ㄹ', '수', '있', '습니다', '.']\n"
          ]
        }
      ],
      "source": [
        "hannanum_tokens = hannanum.morphs(kor_text)\n",
        "print(hannanum_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvV7PzTGehpr"
      },
      "source": [
        "- okt 토큰화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "mSeGgZ5asM7p"
      },
      "outputs": [],
      "source": [
        "# Okt 토큰화\n",
        "from konlpy.tag import Okt\n",
        "okt= Okt()\n",
        "kor_text = \"인간이 컴퓨터와 대화하고 있다는 것을 깨닫지 못하고 인간과 대화를 계속할 수 있다면 컴퓨터는 지능적인 것으로 간주될 수 있습니다.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6ypExXPeuoD",
        "outputId": "2c82c090-74eb-4627-f8da-3843ff06bf7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['인간', '이', '컴퓨터', '와', '대화', '하고', '있다는', '것', '을', '깨닫지', '못', '하고', '인간', '과', '대화', '를', '계속', '할', '수', '있다면', '컴퓨터', '는', '지능', '적', '인', '것', '으로', '간주', '될', '수', '있습니다', '.']\n"
          ]
        }
      ],
      "source": [
        "okt_tokens = okt.morphs(kor_text)\n",
        "print(okt_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drRSUlQQei1Y"
      },
      "source": [
        "- kkma 토큰화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "x2hZhXmisTSi"
      },
      "outputs": [],
      "source": [
        "# Kkma 토큰화\n",
        "from konlpy.tag import Kkma\n",
        "kkma= Kkma()\n",
        "kor_text = \"인간이 컴퓨터와 대화하고 있다는 것을 깨닫지 못하고 인간과 대화를 계속할 수 있다면 컴퓨터는 지능적인 것으로 간주될 수 있습니다.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-KPWhwgeyBK",
        "outputId": "9106c740-9701-4065-be1c-18b89d55f10f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['인간', '이', '컴퓨터', '와', '대화', '하', '고', '있', '다는', '것', '을', '깨닫', '지', '못하', '고', '인간', '과', '대화', '를', '계속', '하', 'ㄹ', '수', '있', '다면', '컴퓨터', '는', '지능', '적', '이', 'ㄴ', '것', '으로', '간주', '되', 'ㄹ', '수', '있', '습니다', '.']\n"
          ]
        }
      ],
      "source": [
        "kkma_tokens = kkma.morphs(kor_text)\n",
        "print(kkma_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbCL0gbKslmD"
      },
      "source": [
        "## 2.2. 한글 품사 부착 (PoS Tagging)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYZMXhgxe1GE"
      },
      "source": [
        "- 코모란 품사 태깅"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZnzQQThsUik",
        "outputId": "727c1335-8671-48ec-ed2a-dfd33fdae491"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('인간', 'NNG'), ('이', 'MM'), ('컴퓨터', 'NNG'), ('오', 'VV'), ('아', 'EC'), ('대화', 'NNG'), ('하', 'NNG'), ('고', 'MM'), ('있', 'VV'), ('달', 'VV'), ('는', 'ETM'), ('것', 'NNB'), ('을', 'NNG'), ('깨닫', 'VV'), ('지', 'NNB'), ('못', 'MAG'), ('하', 'MAG'), ('고', 'MM'), ('인간', 'NNG'), ('과', 'NNG'), ('대화', 'NNG'), ('를', 'JKO'), ('계속', 'MAG'), ('하', 'NNG'), ('ㄹ', 'NA'), ('수', 'NNB'), ('있', 'VV'), ('다면', 'NNG'), ('컴퓨터', 'NNG'), ('늘', 'VV'), ('ㄴ', 'ETM'), ('지능', 'NNP'), ('적', 'NNB'), ('이', 'MM'), ('ㄴ', 'JX'), ('것', 'NNB'), ('으로', 'JKB'), ('간주', 'NNG'), ('되', 'NNB'), ('ㄹ', 'NA'), ('수', 'NNB'), ('있', 'VV'), ('습니다', 'EC'), ('.', 'SF')]\n"
          ]
        }
      ],
      "source": [
        "# 코모란(Komoran) 품사 태깅\n",
        "komoranTag = []\n",
        "for token in komoran_tokens:\n",
        "    komoranTag += komoran.pos(token)\n",
        "print(komoranTag)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mpi4URD8e2cH"
      },
      "source": [
        "- 한나눔 품사 태깅"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_lCDR1zsp1L",
        "outputId": "be575407-a205-4688-ee8d-c61905d9358e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('인간', 'N'), ('이', 'M'), ('컴퓨터', 'N'), ('와', 'I'), ('대화', 'N'), ('하', 'P'), ('고', 'E'), ('있', 'N'), ('다', 'M'), ('는', 'J'), ('것', 'N'), ('을', 'N'), ('깨닫', 'N'), ('지', 'N'), ('못하', 'P'), ('어', 'E'), ('고', 'M'), ('인간', 'N'), ('과', 'N'), ('대화', 'N'), ('를', 'N'), ('계속', 'M'), ('하', 'I'), ('ㄹ', 'N'), ('수', 'N'), ('있', 'N'), ('다면', 'N'), ('컴퓨터', 'N'), ('늘', 'P'), ('ㄴ', 'E'), ('지능적', 'N'), ('이', 'M'), ('ㄴ', 'N'), ('것', 'N'), ('으', 'N'), ('로', 'J'), ('간주', 'N'), ('되', 'N'), ('ㄹ', 'N'), ('수', 'N'), ('있', 'N'), ('슬', 'P'), ('ㅂ니다', 'E'), ('.', 'S')]\n"
          ]
        }
      ],
      "source": [
        "# 한나눔(Hannanum) 품사 태깅\n",
        "hannanumTag = []\n",
        "for token in hannanum_tokens:\n",
        "    hannanumTag += hannanum.pos(token)\n",
        "print(hannanumTag)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Wdri0qVe4EV"
      },
      "source": [
        "- okt 품사 태깅"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXl0KaP0sqr2",
        "outputId": "383b0850-1f99-4a29-b542-03685ebaef58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('인간', 'Noun'), ('이', 'Noun'), ('컴퓨터', 'Noun'), ('와', 'Verb'), ('대화', 'Noun'), ('하고', 'Verb'), ('있다는', 'Adjective'), ('것', 'Noun'), ('을', 'Josa'), ('깨닫지', 'Verb'), ('못', 'Noun'), ('하고', 'Verb'), ('인간', 'Noun'), ('과', 'Noun'), ('대화', 'Noun'), ('를', 'Noun'), ('계속', 'Noun'), ('할', 'Verb'), ('수', 'Noun'), ('있다면', 'Adjective'), ('컴퓨터', 'Noun'), ('는', 'Verb'), ('지능', 'Noun'), ('적', 'Noun'), ('인', 'Noun'), ('것', 'Noun'), ('으로', 'Josa'), ('간주', 'Noun'), ('될', 'Verb'), ('수', 'Noun'), ('있습니다', 'Adjective'), ('.', 'Punctuation')]\n"
          ]
        }
      ],
      "source": [
        "# Okt 품사 태깅\n",
        "oktTag = []\n",
        "for token in okt_tokens:\n",
        "    oktTag += okt.pos(token)\n",
        "print(oktTag)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKWLkuu5e5V5"
      },
      "source": [
        "- kkma 품사 태깅"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gb54VOQVsrpH",
        "outputId": "46894f17-3f71-4219-fad5-f171e7f16b3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('인간', 'NNG'), ('이', 'NNG'), ('컴퓨터', 'NNG'), ('오', 'VA'), ('아', 'ECS'), ('대화', 'NNG'), ('하', 'NNG'), ('고', 'NNG'), ('있', 'VA'), ('달', 'VV'), ('는', 'ETD'), ('것', 'NNB'), ('을', 'NNG'), ('깨닫', 'VV'), ('지', 'NNG'), ('못하', 'VX'), ('고', 'NNG'), ('인간', 'NNG'), ('과', 'NNG'), ('대화', 'NNG'), ('를', 'UN'), ('계속', 'MAG'), ('하', 'NNG'), ('ㄹ', 'NNG'), ('수', 'NNG'), ('있', 'VA'), ('다면', 'NNG'), ('컴퓨터', 'NNG'), ('늘', 'VA'), ('ㄴ', 'ETD'), ('지능', 'NNG'), ('적', 'NNG'), ('이', 'NNG'), ('ㄴ', 'NNG'), ('것', 'NNB'), ('으', 'UN'), ('로', 'JKM'), ('간주', 'NNG'), ('되', 'VA'), ('ㄹ', 'NNG'), ('수', 'NNG'), ('있', 'VA'), ('슬', 'VV'), ('ㅂ니다', 'EFN'), ('.', 'SF')]\n"
          ]
        }
      ],
      "source": [
        "# Kkma 품사 태깅\n",
        "Kkma_tokens = []\n",
        "for token in kkma_tokens:\n",
        "    Kkma_tokens += kkma.pos(token)\n",
        "print(Kkma_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjhQZeLlsv5E"
      },
      "source": [
        "## 2.3. 불용어 처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "J2nhvhWSs0qI"
      },
      "outputs": [],
      "source": [
        "kor_text = \"인간이 컴퓨터와 대화하고 있다는 것을 깨닫지 못하고 인간과 대화를 계속할 수 있다면 컴퓨터는 지능적인 것으로 간주될 수 있습니다.\"\n",
        "\n",
        "# 불용어 리스트 정의\n",
        "stopwords = ['이', '있', '하', '것', '들', '그', '되', '수', '않', \\\n",
        "             '없', '나', '우리', '가', '한', '같', '때', '년', '에', \\\n",
        "             '와', '고', '로', '를', '으로', '에게', '및', '의', '를', \\\n",
        "             '은', '는', '에', '도', '가', '을', '이다', '다']\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOtDvCVkfAvq",
        "outputId": "2e33069f-1143-4cbd-df13-5d3173b23125"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['인간',\n",
              " '이',\n",
              " '컴퓨터',\n",
              " '와',\n",
              " '대화',\n",
              " '하고',\n",
              " '있다는',\n",
              " '것',\n",
              " '을',\n",
              " '깨닫지',\n",
              " '못',\n",
              " '하고',\n",
              " '인간',\n",
              " '과',\n",
              " '대화',\n",
              " '를',\n",
              " '계속',\n",
              " '할',\n",
              " '수',\n",
              " '있다면',\n",
              " '컴퓨터',\n",
              " '는',\n",
              " '지능',\n",
              " '적',\n",
              " '인',\n",
              " '것',\n",
              " '으로',\n",
              " '간주',\n",
              " '될',\n",
              " '수',\n",
              " '있습니다',\n",
              " '.']"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 텍스트를 형태소 단위로 토큰화\n",
        "okt_tokens = Okt().morphs(kor_text)\n",
        "okt_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlTsOh9ofDoG",
        "outputId": "b6dc6d26-8ce6-4bcc-fa6e-2dd7552ac5bc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['인간',\n",
              " '컴퓨터',\n",
              " '대화',\n",
              " '하고',\n",
              " '있다는',\n",
              " '깨닫지',\n",
              " '못',\n",
              " '하고',\n",
              " '인간',\n",
              " '과',\n",
              " '대화',\n",
              " '계속',\n",
              " '할',\n",
              " '있다면',\n",
              " '컴퓨터',\n",
              " '지능',\n",
              " '적',\n",
              " '인',\n",
              " '간주',\n",
              " '될',\n",
              " '있습니다',\n",
              " '.']"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 불용어 제거\n",
        "filtered_tokens = [word for word in okt_tokens if word not in stopwords]\n",
        "filtered_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFquQ4BifEcU",
        "outputId": "38500f56-2679-40f4-a9ae-b05bd04f4d70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "형태소 분석 결과: ['인간', '이', '컴퓨터', '와', '대화', '하고', '있다는', '것', '을', '깨닫지', '못', '하고', '인간', '과', '대화', '를', '계속', '할', '수', '있다면', '컴퓨터', '는', '지능', '적', '인', '것', '으로', '간주', '될', '수', '있습니다', '.']\n",
            "불용어 제거 후 토큰: ['인간', '컴퓨터', '대화', '하고', '있다는', '깨닫지', '못', '하고', '인간', '과', '대화', '계속', '할', '있다면', '컴퓨터', '지능', '적', '인', '간주', '될', '있습니다', '.']\n"
          ]
        }
      ],
      "source": [
        "# 결과 출력\n",
        "print(\"형태소 분석 결과:\", okt_tokens)\n",
        "print(\"불용어 제거 후 토큰:\", filtered_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gensim import corpora, models\n",
        "\n",
        "dictionary = corpora.Dictionary(texts) # 각 단어에 id부여 {'단어1': 0(단어1 id), '단어2':1(단어2 id) ....} dictionary.token2id로 확인\n",
        "\n",
        "# BOW (Bag of Words) 코퍼스 생성 - 백터화\n",
        "corpus = [dictionary.doc2bow(text) for text in texts] # 각 기사별 단어의 빈도수 [(단어 id, 빈도수 ), (단어 id, 빈도수)...]\n",
        "\n",
        "# 지정할 토픽 개수\n",
        "num_topics = 2\n",
        "\n",
        "# LDA 모델 학습\n",
        "lda_model = models.LdaModel(corpus,                \n",
        "                            num_topics=num_topics, # 모델이 학습할 주제 수(뉴스는 10~20개 적절)\n",
        "                            id2word=dictionary,    # 말뭉치의 단어 id를 실제 단어와 매칭\n",
        "                            passes=10,             # 학습 횟수\n",
        "                            random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dictionary.token2id # {'단어':'단어ID'....}  확인\n",
        "corpus # [(단어id,빈도수),(단어id,빈도수)...] 확인\n",
        "# lda_model.show_topic(LDA 토픽 번호, topn=상위 키워드 몇개 볼건지) # 토픽별 많이 나온 단어\n",
        "lda_model[corpus[0]] # 0번 글이 어느 토픽을 더 잘 설명하고 있는지 [(0, 0.9093107), (1, 0.090689294)] 0토픽 90% 설명 1토픽 9% 설명"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "상위 토픽 단어 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 상위 토픽 담을 곳\n",
        "topic_keywords = {}\n",
        "\n",
        "# 각 토픽별 주요 키워드 추출\n",
        "for topic_id in range(num_topics):\n",
        "    keywords = lda_model.show_topic(topic_id, topn=topn_keywords) # 토픽별 가장 많이 나온 단어(토픽번호, 실제로 볼 키워드 개수)\n",
        "    topic_keywords[topic_id] = keywords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "관련성 높은 뉴스 가져오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''\n",
        "dominant_topic 설명   \n",
        "1. topic_distribution의 형태는 [(0, 0.9093107), (1, 0.090689294)] 이런식으로 나옴    \n",
        "2. key식을 적용해서 0.9093107와 0.090689294중 max를 비교하고 더 큰 (0, 0.9093107)를 선택    \n",
        "3. 맨 뒤에 [0]는 선택된 (0, 0.9093107)에서 토픽 번호인 0을 가져옴\n",
        "'''\n",
        "# 토픽 id 나누기\n",
        "topic_articles = {i: [] for i in range(num_topics)}\n",
        "\n",
        "# corpus = 각 기사의 [(단어id, 단어 빈도)....]\n",
        "for idx, doc in enumerate(corpus):      # idx = 각 기사의 번호부여, doc= 기사 1개\n",
        "    topic_distribution = lda_model[doc] # 해당 기사의 토픽 관련도 [(1, 0.9992884)] 는 1토픽과 99% 관련된 기사라는 뜻\n",
        "    dominant_topic = max(topic_distribution, key=lambda x: x[1])  # 가장 높은 가중치를 가진 토픽 (1, 0.9992884)\n",
        "    topic_articles[dominant_topic[0]].append({dominant_topic[0] : news_data[idx]}) # 더 관련성 있는 토픽에 {관련성 : 기사}를 추가\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 연관도 순으로 기사 가져오는 코드\n",
        "def get_topic_articles(lda_model, corpus, news_data, num_topics):\n",
        "    topic_articles = {i: [] for i in range(num_topics)}\n",
        "\n",
        "    # 가장 관련성 높은 토픽 dict에 (토픽관련성 : 기사내용)형식으로 삽입\n",
        "    for idx, doc in enumerate(corpus):\n",
        "        topic_distribution = lda_model[doc]\n",
        "        dominant_topic = max(topic_distribution, key=lambda x: x[1])  # 관련이 높은 토픽과 관련도(토픽id, 관련도)\n",
        "        topic_articles[dominant_topic[0]].append({dominant_topic[1] : news_data[idx]})\n",
        "    \n",
        "    # 관련도 순으로 정렬\n",
        "    for topic in range(num_topics):\n",
        "        topic_articles[topic] = sorted(topic_articles[topic], key = lambda x: list(x.keys()), reverse=True)\n",
        "\n",
        "    return topic_articles\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "study",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
